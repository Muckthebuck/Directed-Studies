{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-calculate the underwater params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pude_training_loop.loss_functions_torch as loss_functions\n",
    "import Pude_training_loop.pude_utils as pude_utils\n",
    "import Pude_training_loop.model_training as model_training\n",
    "import Pude_training_loop.dataset_loader as data_loader\n",
    "from Pude_training_loop.data_logger import Data_logger\n",
    "from Pude_training_loop.physics_parameter_estmation import UnderwaterParameterFinder\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_log_results_dir = \"Results/SeaThru_Combined/Datalogger_params/\"\n",
    "\n",
    "model_name = \"depth_anything\"\n",
    "\n",
    "depth_anything_model, depth_anything_image_processor = model_training.get_model_image_processor_pair(model_name=model_name, model_path=model_training.models[model_name], device=device)\n",
    "\n",
    "dataset_loader = data_loader.DatasetLoader() # Initialize dataset loader with default parameters\n",
    "# for pude tau_thresholds=(0.08, 0.6), depthanyting its 1\n",
    "underwater_parameter_estimator = UnderwaterParameterFinder() # Initialize underwater parameter finder with default parameters\n",
    "data_logger = Data_logger(results_dir=data_log_results_dir)\n",
    "# Define training parameters\n",
    "batch_size = 1\n",
    "\n",
    "# seed the torch\n",
    "torch.manual_seed(model_training.seed)\n",
    "\n",
    "# storing underwater params for efficient training\n",
    "underwater_params = []\n",
    "\n",
    "\n",
    "is_first_epoch = True\n",
    "skipped_images = []\n",
    "max_images_to_process = 182\n",
    "\n",
    "for i in range(len(dataset_loader)):\n",
    "    non_linear_images, linear_images = dataset_loader[i]\n",
    "    # linear_images = linear_images.to(device)\n",
    "    # Similarly, convert non_linear_images to a PyTorch tensor\n",
    "    non_linear_images_tensor = torch.tensor(non_linear_images)\n",
    "    # Forward pass\n",
    "    depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "                                                            image_processor=depth_anything_image_processor, \n",
    "                                                            raw_image=non_linear_images_tensor, device=device, requires_grad=False)\n",
    "    \n",
    "    # normalize the depth map\n",
    "    model_output = depth_anything_output.cpu().detach().numpy()\n",
    "    depth_anything_output =(((model_output-np.min(model_output)) / (np.max(model_output)-np.min(model_output)))*20.0)\n",
    "\n",
    "    hat_nu, hat_mu, hat_B_infty, valid_estimate = underwater_parameter_estimator.algorithm_1(d_D=depth_anything_output, I=linear_images, data_logger=data_logger, non_linear_image=non_linear_images)\n",
    "    print(\"Estimated parameters: \", hat_nu, hat_mu, hat_B_infty, valid_estimate)\n",
    "    data_logger.log_data((hat_nu, hat_mu, hat_B_infty, valid_estimate))\n",
    "\n",
    "\n",
    "data_logger.save_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated parameters:  [9.56667519 4.6959245 ] 1.0 [0.20937552 0.35893391] True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New PUDE Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pude_training_loop.loss_functions_torch as loss_functions\n",
    "import Pude_training_loop.pude_utils as pude_utils\n",
    "import Pude_training_loop.model_training as model_training\n",
    "import Pude_training_loop.dataset_loader as data_loader\n",
    "from Pude_training_loop.physics_parameter_estmation import UnderwaterParameterFinder\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for alpha3 in [2,4,6]:\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"Training for alpha3: {alpha3}\")\n",
    "    # seed the torch\n",
    "    torch.manual_seed(model_training.seed)\n",
    "    np.random.seed(model_training.seed)\n",
    "\n",
    "    # depth_anything_model, depth_anything_image_processor = model_training.get_model_image_processor_pair(model_name=\"depth_anything\", model_path=model_training.models[\"depth_anything\"], device=device)\n",
    "    # new_pude_model, new_pude_mode_image_processor = model_training.get_model_image_processor_pair(model_name=\"depth_anything\", model_path=model_training.models[\"depth_anything\"], device=device)\n",
    "    depth_anything_model, new_pude_model, depth_anything_image_processor, new_pude_mode_image_processor = model_training.get_two_separate_model_pairs(model_path=model_training.models[\"depth_anything\"], device=device)\n",
    "    dataset_loader = data_loader.DatasetLoader() # Initialize dataset loader with default parameters #1e-15 smoothness\n",
    "    # betas = [1, 0.1, 0], alphas = [1, 0.1, 4]\n",
    "    pude_loss_fn = loss_functions.PUDELoss(betas=[1, 0.1,0], alphas=[1, 0.1, alpha3]) # Initialize Pude loss function with default parameters\n",
    "    # 1, 0.1 \n",
    "    # pude_loss_fn = loss_functions.PUDELoss(betas=[20, 60], alphas=[1,0.1,1]) # Initialize Pude loss function with default parameters\n",
    "    # pude_loss_fn = loss_functions.PUDELoss(betas=[5, 0.0005]) # Initialize Pude loss function with default parameters\n",
    "    # Define training parameters\n",
    "    # epochs = 3\n",
    "    # learning_rate = 1e-6\n",
    "    # batch_size = 1\n",
    "\n",
    "    # # Define optimizer\n",
    "    # optimizer = torch.optim.AdamW(new_pude_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    new_pude_model.train()\n",
    "\n",
    "\n",
    "    # Define training parameterss\n",
    "    epochs = 3\n",
    "    learning_rate = 1e-6\n",
    "    batch_size = 1\n",
    "\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.AdamW(new_pude_model.parameters(), lr=learning_rate)\n",
    "    # linear scheduler \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Training started\")\n",
    "\n",
    "    #  load the underwater params from pickle file\n",
    "    # underwater_param_file = \"Results/SeaThru_Combined/Datalogger_params/params/parameter_results.pickle\"\n",
    "    # underwater_params = pude_utils.load_pickle(underwater_param_file)\n",
    "    # # skipped images for seathrunerf\n",
    "    # # skipped_images = [5, 14, 25, 59, 60, 61]\n",
    "    # # skipped images for seathrucombined\n",
    "    # skipped_images = [5,  14, 23, 24, 25, 28, 37, 38, 39, 41, \n",
    "    #                   42, 43, 45, 46, 51, 52, 53, 56, 57, 59, \n",
    "    #                   61, 63, 66, 67, 69, 70, 74, 75, 81, 82, \n",
    "    #                   83, 86, 87, 88, 95, 96, 118, 171, 172, 173, \n",
    "    #                   175, 177, 179 ]\n",
    "\n",
    "\n",
    "    #  load the underwater params from pickle file\n",
    "    underwater_param_file = \"Results/SeaThru_Combined/Datalogger_params/params/parameter_results.pickle\"\n",
    "    underwater_params = pude_utils.load_pickle(underwater_param_file)\n",
    "    skipped_images = [5,  14, 23, 24, 25, 28, 37, 38, 39, 41, \n",
    "                    42, 43, 45, 46, 51, 52, 53, 55, 56, 57, 59, \n",
    "                    61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73,  74, 75, 76, 77, 78, 81, 82, \n",
    "                    83, 85,  86, 87, 88, 89, 90, 93,94,  95, 96, 97, 107, 108, 112, 113, 118, 119, 122, 123, 124, 127, 130, 131, 171, 172, 173, 174,\n",
    "                    175, 177, 179 ]\n",
    "\n",
    "\n",
    "    max_images_to_process = len(dataset_loader)-len(skipped_images)\n",
    "    print(\"Total images to process: \", max_images_to_process)\n",
    "\n",
    "    indices = np.random.permutation(len(dataset_loader))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        n = 0\n",
    "        for i in indices:\n",
    "            # if (i+1 - len(skipped_images))>max_images_to_process:\n",
    "            #     break\n",
    "            if i in skipped_images:\n",
    "                continue\n",
    "            n+=1\n",
    "            \n",
    "            non_linear_images, linear_images = dataset_loader[i]\n",
    "            # linear_images = linear_images.to(device)\n",
    "            # Similarly, convert non_linear_images to a PyTorch tensor\n",
    "            non_linear_images_tensor = torch.tensor(non_linear_images)\n",
    "            # Forward pass\n",
    "            depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "                                                                    image_processor=depth_anything_image_processor, \n",
    "                                                                    raw_image=non_linear_images_tensor, device=device, requires_grad=False)\n",
    "            pude_output = model_training.get_model_output(model=new_pude_model, \n",
    "                                                        image_processor=new_pude_mode_image_processor, \n",
    "                                                        raw_image=non_linear_images_tensor, device=device)\n",
    "            # parameter estimation\n",
    "            hat_nu, hat_mu, hat_B_infty, _ = underwater_params[i]\n",
    "\n",
    "            # if (n)%1==0 or n==1:\n",
    "            #     result_images = pude_utils.pude_display_image_with_depth(image=Image.fromarray(non_linear_images), \n",
    "            #                                                              depth1=depth_anything_output.cpu().detach().numpy(), \n",
    "            #                                                              depth2=pude_output.cpu().detach().numpy())\n",
    "            #     result_images.save(f\"Results/SeaThru_Combined/Pude_results/epoch{epoch}_{n}_image_{i}.png\")\n",
    "            # Loss calculation\n",
    "            loss = pude_loss_fn( pude_output, depth_anything_output, \n",
    "                                torch.tensor(linear_images, device=device, requires_grad=True), \n",
    "                                torch.tensor(hat_nu, device=device, requires_grad=True), \n",
    "                                torch.tensor(hat_mu, device=device, requires_grad=True), \n",
    "                                torch.tensor(hat_B_infty, device=device, requires_grad=True).unsqueeze(1))\n",
    "            if loss.item() is torch.nan:\n",
    "                print(f\"Loss is nan for image {i}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((n)%20==0  or n==1):\n",
    "                print(f\"Epoch: {epoch}, Image: {n}, Loss: {loss.item()}\")\n",
    "                eval_image = model_training.evaluate(depth_anything_model=depth_anything_model, new_pude_model=new_pude_model,\n",
    "                                depth_anything_image_processor=depth_anything_image_processor, new_pude_image_processor=new_pude_mode_image_processor,\n",
    "                                non_linear_images=dataset_loader[-1][0]) \n",
    "                display(eval_image)      \n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        scheduler.step()\n",
    "        # Save the model\n",
    "        # torch.save(new_pude_model.state_dict(), f\"new_pude_model_combined_edge_aware_betas_1_1_1_alphas_1_0-1_1_lr_{str(learning_rate)}_epoch_{str(epoch)}.pth\")\n",
    "        \n",
    "\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    print(f\"Number of skipped images: {len(skipped_images)}\")\n",
    "    # print(f\"Number of processed images: {min(60,len(dataset_loader)-len(skipped_images))}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Save the model\n",
    "    # torch.save(new_pude_model.state_dict(), \"new_pude_model_2.pth\")\n",
    "\n",
    "    def plot_one_over_z_vs_d(actual_depth, model_output, save_folder=None, img_name=\"test image\"):\n",
    "        # Remove zero values\n",
    "        z = actual_depth.flatten()\n",
    "        d = model_output.flatten()\n",
    "\n",
    "        # normalise d\n",
    "        d = d/np.max(d)\n",
    "        z = z/np.max(z)\n",
    "\n",
    "\n",
    "        non_zero = np.where(z > 0.15)\n",
    "        z = 1/z[non_zero]\n",
    "        d = d[non_zero]\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        def _fit_lingress(z,d):\n",
    "            # Linear regression\n",
    "            result = linregress(z, d)\n",
    "            \n",
    "            # Line of best fit\n",
    "            fit_x = np.linspace(np.min(z), np.max(z), 100)\n",
    "            fit_y = result.slope * fit_x + result.intercept\n",
    "            return result, fit_x, fit_y\n",
    "            \n",
    "        def _remove_outlier(z,d):\n",
    "            # Linear regression\n",
    "            result, fit_x, fit_y = _fit_lingress(z,d)\n",
    "            # Calculate residuals\n",
    "            residuals = d - result.slope * z - result.intercept\n",
    "\n",
    "            abs_residuals = np.abs(residuals)\n",
    "            threshold = 4 * np.std(abs_residuals)\n",
    "\n",
    "            # Identify the non-outliers\n",
    "            non_outliers_mask = abs_residuals < threshold\n",
    "\n",
    "            return z[non_outliers_mask], d[non_outliers_mask]\n",
    "\n",
    "\n",
    "        z, d = _remove_outlier(z,d)\n",
    "        result, fit_x, fit_y = _fit_lingress(z,d)\n",
    "\n",
    "        # Scatter plot\n",
    "        plt.scatter(z, d, s=1)\n",
    "        plt.xlabel(\"1/z\")\n",
    "        plt.ylabel(\"d\")\n",
    "    \n",
    "        plt.plot(fit_x, fit_y, '-r', label='Line of best fit, r = {:.3f}'.format(result.rvalue))\n",
    "        plt.ylim(bottom=0)\n",
    "        # Display R-squared value\n",
    "        plt.legend()\n",
    "        plt.title(f\"1/z vs d for {img_name}\")\n",
    "        plt.show( )\n",
    "        # plt.savefig(save_folder, dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return result.rvalue\n",
    "    # correct image\n",
    "    def correct_image(img):\n",
    "        \"\"\"\n",
    "        Correct the brightness of an RGB image.\n",
    "\n",
    "        Parameters:\n",
    "            img (numpy.ndarray): Input RGB image.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Image with corrected brightness.\n",
    "        \"\"\"\n",
    "        # Get the shape of the image\n",
    "        rows, cols, channels = img.shape\n",
    "\n",
    "        # Compute the average brightness across all channels\n",
    "        brightness = np.sum(img) / (255 * rows * cols * channels)\n",
    "\n",
    "        # Define the target minimum brightness\n",
    "        minimum_brightness = 0.3\n",
    "\n",
    "        # Compute the brightness ratio\n",
    "        ratio = brightness / minimum_brightness\n",
    "\n",
    "        # If the ratio is greater than or equal to 1, the image is already bright enough\n",
    "        if ratio >= 1:\n",
    "            print(\"Image already bright enough\")\n",
    "            return img\n",
    "\n",
    "        # Otherwise, adjust brightness to get the target brightness for each channel\n",
    "        corrected_img = cv2.convertScaleAbs(img, alpha=1 / ratio, beta=0)\n",
    "\n",
    "        return corrected_img\n",
    "\n",
    "\n",
    "    def white_balance_linear(img):\n",
    "        # Compute the average value of each color channel\n",
    "\n",
    "        # Get the shape of the image\n",
    "        rows, cols, channels = img.shape\n",
    "\n",
    "        # Compute the average brightness across all channels\n",
    "        brightness = np.sum(img) / (255 * rows * cols * channels)\n",
    "\n",
    "        # Define the target minimum brightness\n",
    "        minimum_brightness = 0.25\n",
    "\n",
    "        # Compute the brightness ratio\n",
    "        ratio = brightness / minimum_brightness\n",
    "\n",
    "        # If the ratio is greater than or equal to 1, the image is already bright enough\n",
    "        if ratio >= 1:\n",
    "            print(\"Image already bright enough\")\n",
    "            return img\n",
    "\n",
    "        # Clip the scaled image to ensure pixel values remain in the valid range [0, 255]\n",
    "        corrected_img = np.clip(img * 1/ratio, 0, 255)\n",
    "\n",
    "        # # Otherwise, adjust brightness to get the target brightness for each channel\n",
    "        # corrected_img = cv2.convertScaleAbs(img, alpha=1 / ratio, beta=0)\n",
    "\n",
    "        return corrected_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "    def _prepare_depth(model_output):\n",
    "            # prepare images for visualization\n",
    "            # format the image to be between 0 and 255\n",
    "            formatted = (((model_output-np.min(model_output)) / (np.max(model_output)-np.min(model_output)))*255).astype(\"uint8\")\n",
    "            colored_depth = cv2.applyColorMap(formatted, cv2.COLORMAP_INFERNO)[:, :, ::-1]\n",
    "            depth = Image.fromarray(colored_depth)\n",
    "            return depth\n",
    "    \n",
    "\n",
    "    Dataset_name = \"D5\"\n",
    "    image_name = \"LFT_3395\"\n",
    "\n",
    "    # Dataset_name = \"D3\"\n",
    "    # image_name = \"T_S04857\"\n",
    "    test_image_path = f\"Datasets\\SeaThru\\{Dataset_name}\\linearPNG\\{image_name}.png\"\n",
    "    # test_image_path = f\"Datasets\\SeaThru_old\\{Dataset_name}\\Raw\\{image_name}.NEF\"\n",
    "    # test_image_path = f\"Datasets\\SeaThru_old\\{Dataset_name}\\Raw\\{image_name}.ARW\"\n",
    "    # test_image_path = f\"Datasets\\SeaThruNeRF\\Curasao\\images_wb\\MTN_1288.png\"\n",
    "    ground_truth_path = f\"Datasets\\SeaThru\\{Dataset_name}\\depth\\depth{image_name}.tif\"\n",
    "    actual_depth = dataset_loader.open_depth_map(path=ground_truth_path, img_dim=(model_training.default_image_dim[1], model_training.default_image_dim[0]))\n",
    "    non_linear_image = dataset_loader._open_image(test_image_path, img_dim=model_training.default_image_dim)[0]\n",
    "\n",
    "    non_linear_image_np = correct_image(non_linear_image)\n",
    "    # non_linear_image_np = non_linear_image\n",
    "    non_linear_image = Image.fromarray(non_linear_image_np)\n",
    "\n",
    "    display(pude_utils.make_image_grid([non_linear_image, _prepare_depth(actual_depth)], rows=1, cols=2))\n",
    "\n",
    "    non_linear_image_tensor = torch.tensor(non_linear_image_np)\n",
    "\n",
    "    pude_output = model_training.get_model_output(model=new_pude_model, \n",
    "                                                        image_processor=new_pude_mode_image_processor, \n",
    "                                                        raw_image=non_linear_image_tensor, device=device, requires_grad=False)\n",
    "    depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "                                                                    image_processor=depth_anything_image_processor, \n",
    "                                                                    raw_image=non_linear_image_tensor, device=device, requires_grad=False)\n",
    "    pude_utils.display_image_with_depth(image=_prepare_depth(actual_depth), depth1=depth_anything_output.cpu().detach().numpy(), depth2=pude_output.cpu().detach().numpy())\n",
    "\n",
    "    plot_one_over_z_vs_d(actual_depth, pude_output.cpu().detach().numpy(), img_name=\"pude_output\")\n",
    "    plot_one_over_z_vs_d(actual_depth, depth_anything_output.cpu().detach().numpy(), img_name=\"depth_anything_output\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_pude_model.state_dict(), \"best_new_pude_model_Adam_betas_1_0.1_0_alphas_0.5_0.1_4_lr_1e-6_epoch_3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New stereo + pude training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pude_training_loop.loss_functions_torch as loss_functions\n",
    "import Pude_training_loop.pude_utils as pude_utils\n",
    "import Pude_training_loop.model_training as model_training\n",
    "import Pude_training_loop.dataset_loader as data_loader\n",
    "from monocular_stereo_matching.stereo_pair_gen import Stereo_Pair_Generator\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "pude_path = \"best_new_pude_model_Adam_betas_1_0.1_0_alphas_0.5_0.1_4_lr_1e-6_epoch_3.pth\"\n",
    "\n",
    "unimatch_model, unimatch_image_processor = model_training.get_model_image_processor_pair(model_name=\"unimatch\", model_path=model_training.models[\"unimatch\"], device=device)\n",
    "depth_anything_model, depth_anything_image_processor = model_training.get_model_image_processor_pair(model_name=\"new_pude\", model_path=pude_path, device=device)\n",
    "# depth_anything_model, new_pude_model, depth_anything_image_processor, new_pude_mode_image_processor = model_training.get_two_separate_model_pairs(model_path=model_training.models[\"depth_anything\"], device=device)\n",
    "dataset_loader = data_loader.DatasetLoader() # Initialize dataset loader with default parameters\n",
    "loss_fn = loss_functions.PUDELoss(betas=[1, 20, 4], alphas=[1,0.1,4]) # Initialize Pude loss function with default parameters\n",
    "#loss_fn = loss_functions.PUDELoss(betas=[20, 60], alphas=[1,0.1,1]) # Initialize Pude loss function with default parameters\n",
    "#loss_fn = loss_functions.PUDELoss(betas=[6, 0.0005]) # Initialize Pude loss function with default parameters\n",
    "stereo_pair_gen = Stereo_Pair_Generator(image_dim=model_training.default_image_dim)\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 5\n",
    "learning_rate = 5e-6\n",
    "batch_size = 1\n",
    "\n",
    "unimatch_model.train()\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(unimatch_model.parameters(), lr=learning_rate)\n",
    "# scheduler \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "\n",
    "\n",
    "# seed the torch\n",
    "torch.manual_seed(model_training.seed)\n",
    "np.random.seed(model_training.seed)\n",
    "\n",
    "print(\"Training started\")\n",
    "\n",
    "# unimatch_outputs = []\n",
    "\n",
    "#  load the underwater params from pickle file\n",
    "underwater_param_file = \"Results/SeaThru_Combined/Datalogger_params/params/parameter_results.pickle\"\n",
    "underwater_params = pude_utils.load_pickle(underwater_param_file)\n",
    "skipped_images = [5,  14, 23, 24, 25, 28, 37, 38, 39, 41, \n",
    "                  42, 43, 45, 46, 51, 52, 53, 55, 56, 57, 59, \n",
    "                  61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73,  74, 75, 76, 77, 78, 81, 82, \n",
    "                  83, 85,  86, 87, 88, 89, 90, 93,94,  95, 96, 97, 107, 108, 112, 113, 118, 119, 122, 123, 124, 127, 130, 131, 171, 172, 173, 174,\n",
    "                  175, 177, 179 ]\n",
    "\n",
    "print(\"Total images to process: \", len(dataset_loader)-len(skipped_images))\n",
    "\n",
    "# create a random list of indices to shuffle the images in the dataset\n",
    "indices = np.random.permutation(len(dataset_loader))\n",
    "\n",
    "# max_images_to_process = 64\n",
    "for epoch in range(epochs):\n",
    "    n=0\n",
    "    for i in indices:\n",
    "        # if (i+1 - len(skipped_images))>max_images_to_process:\n",
    "        #     break\n",
    "        if i in skipped_images:\n",
    "            continue\n",
    "        n+=1\n",
    "        optimizer.zero_grad()\n",
    "        non_linear_images, linear_images = dataset_loader[i]\n",
    "        # linear_images = linear_images.to(device)\n",
    "        # Similarly, convert non_linear_images to a PyTorch tensor\n",
    "        non_linear_images_tensor = torch.tensor(non_linear_images)\n",
    "        # Forward pass\n",
    "        depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "                                                                image_processor=depth_anything_image_processor, \n",
    "                                                                raw_image=non_linear_images_tensor, device=device, requires_grad=False)\n",
    "        # parameter estimation\n",
    "        # depth_anything_output =(((depth_anything_output-torch.min(depth_anything_output)) / (torch.max(depth_anything_output)-torch.min(depth_anything_output)))*20.0)\n",
    "\n",
    "        # for pude model rescale it between 0 and 20\n",
    "       \n",
    "        # generate stereo pair #depth anything 1.5 scaling #pude 0.005 scaling\n",
    "        scaling_factor = np.random.uniform(0.5, 5)\n",
    "        # scaling_factor = np.random.randint(0.8, 5) # [2, 5)\n",
    "        image_2 = stereo_pair_gen.generate_stereo_pair(non_linear_images, depth_anything_output.cpu().detach().numpy(), scaling_factor=scaling_factor)\n",
    "    \n",
    "        unimatch_input = {\"image1\": non_linear_images, \"image2\": image_2}\n",
    "        unimatch_output = model_training.get_model_output(model=unimatch_model, \n",
    "                                                     image_processor=unimatch_image_processor, \n",
    "                                                     raw_image=unimatch_input, device=device)\n",
    "        \n",
    "        # unimatch_outputs.append(unimatch_output.cpu().detach().numpy())\n",
    "        if (n)%1==0 or n==1:\n",
    "            result_images = pude_utils.unimatch_display_image_with_depth(image=Image.fromarray(non_linear_images),\n",
    "                                                            depth1=depth_anything_output.cpu().detach().numpy(),\n",
    "                                                            depth2=unimatch_output.cpu().detach().numpy(),\n",
    "                                                            shifted_image=Image.fromarray(image_2), scaling_factor=scaling_factor)\n",
    "            # save the image\n",
    "            result_images.save(f\"Results/SeaThru_Combined/Unimatch_results/epoch{epoch}_{n}_image_{i}.png\")\n",
    "    \n",
    "        hat_nu, hat_mu, hat_B_infty, _ = underwater_params[i]\n",
    "        # Loss calculation\n",
    "        loss = loss_fn( unimatch_output, depth_anything_output, \n",
    "                            torch.tensor(linear_images, device=device, requires_grad=True), \n",
    "                            torch.tensor(hat_nu, device=device, requires_grad=True), \n",
    "                            torch.tensor(hat_mu, device=device, requires_grad=True), \n",
    "                            torch.tensor(hat_B_infty, device=device, requires_grad=True).unsqueeze(1))\n",
    "        \n",
    "        # # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if ((n)%50==0):\n",
    "            eval_image = model_training.evaluate_unimatch(depth_anything_model=depth_anything_model, unimatch_model=unimatch_model,\n",
    "                            depth_anything_image_processor=depth_anything_image_processor, unimatch_image_processor=unimatch_image_processor,\n",
    "                            stereo_pair_gen=stereo_pair_gen,non_linear_images=dataset_loader[-1][0], scaling_factor=2) \n",
    "            display(eval_image)      \n",
    "    scheduler.step()\n",
    "    torch.save(unimatch_model.state_dict(), f\"new_unimatch_model_edge_aware_loss_cosine_scheduler_AdamW_betas_1_20_4_alphas_1_0.1_4_lr_{str(learning_rate)}_epoch_{str(epoch)}.pth\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    # evaluate the model and show image grid\n",
    "    eval_image = model_training.evaluate_unimatch(depth_anything_model=depth_anything_model, unimatch_model=unimatch_model,\n",
    "                depth_anything_image_processor=depth_anything_image_processor, unimatch_image_processor=unimatch_image_processor,\n",
    "                stereo_pair_gen=stereo_pair_gen,non_linear_images=dataset_loader[-1][0], scaling_factor=2) \n",
    "    display(eval_image)\n",
    "    \n",
    "# save the unimatch outputs as pickle using pickle \n",
    "# pickle_file = \"Results/SeaThru_Combined/unimatch_outputs.pickle\"\n",
    "# with open(pickle_file, 'wb') as f:\n",
    "#     pickle.dump(unimatch_outputs, f)\n",
    "\n",
    "print(\"Training completed\")\n",
    "print(f\"Number of skipped images: {len(skipped_images)}\")\n",
    "# print(f\"Number of processed images: {min(60,len(dataset_loader)-len(skipped_images))}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "# torch.save(new_pude_model.state_dict(), \"new_pude_model_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unimatch_model.state_dict(), \"new_unimatch_model_cosine_scheduler_AdamW_betas_60_60,_alphas_4_0-4_8_lr_5e-6_epochs_3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_new_pude():\n",
    "    import cv2\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from scipy.stats import linregress\n",
    "    from PIL import Image\n",
    "\n",
    "\n",
    "    def plot_one_over_z_vs_d(actual_depth, model_output, save_folder=None, img_name=\"test image\"):\n",
    "        # Remove zero values\n",
    "        z = actual_depth.flatten()\n",
    "        d = model_output.flatten()\n",
    "\n",
    "        # normalise d\n",
    "        d = d/np.max(d)\n",
    "        z = z/np.max(z)\n",
    "\n",
    "\n",
    "        non_zero = np.where(z > 0.15)\n",
    "        z = 1/z[non_zero]\n",
    "        d = d[non_zero]\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        def _fit_lingress(z,d):\n",
    "            # Linear regression\n",
    "            result = linregress(z, d)\n",
    "            \n",
    "            # Line of best fit\n",
    "            fit_x = np.linspace(np.min(z), np.max(z), 100)\n",
    "            fit_y = result.slope * fit_x + result.intercept\n",
    "            return result, fit_x, fit_y\n",
    "            \n",
    "        def _remove_outlier(z,d):\n",
    "            # Linear regression\n",
    "            result, fit_x, fit_y = _fit_lingress(z,d)\n",
    "            # Calculate residuals\n",
    "            residuals = d - result.slope * z - result.intercept\n",
    "\n",
    "            abs_residuals = np.abs(residuals)\n",
    "            threshold = 4 * np.std(abs_residuals)\n",
    "\n",
    "            # Identify the non-outliers\n",
    "            non_outliers_mask = abs_residuals < threshold\n",
    "\n",
    "            return z[non_outliers_mask], d[non_outliers_mask]\n",
    "\n",
    "\n",
    "        z, d = _remove_outlier(z,d)\n",
    "        result, fit_x, fit_y = _fit_lingress(z,d)\n",
    "\n",
    "        # Scatter plot\n",
    "        plt.scatter(z, d, s=1)\n",
    "        plt.xlabel(\"1/z\")\n",
    "        plt.ylabel(\"d\")\n",
    "    \n",
    "        plt.plot(fit_x, fit_y, '-r', label='Line of best fit, r = {:.3f}'.format(result.rvalue))\n",
    "        plt.ylim(bottom=0)\n",
    "        # Display R-squared value\n",
    "        plt.legend()\n",
    "        plt.title(f\"1/z vs d for {img_name}\")\n",
    "        plt.show( )\n",
    "        # plt.savefig(save_folder, dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return result.rvalue\n",
    "    # correct image\n",
    "    def correct_image(img):\n",
    "        \"\"\"\n",
    "        Correct the brightness of an RGB image.\n",
    "\n",
    "        Parameters:\n",
    "            img (numpy.ndarray): Input RGB image.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Image with corrected brightness.\n",
    "        \"\"\"\n",
    "        # Get the shape of the image\n",
    "        rows, cols, channels = img.shape\n",
    "\n",
    "        # Compute the average brightness across all channels\n",
    "        brightness = np.sum(img) / (255 * rows * cols * channels)\n",
    "\n",
    "        # Define the target minimum brightness\n",
    "        minimum_brightness = 0.3\n",
    "\n",
    "        # Compute the brightness ratio\n",
    "        ratio = brightness / minimum_brightness\n",
    "\n",
    "        # If the ratio is greater than or equal to 1, the image is already bright enough\n",
    "        if ratio >= 1:\n",
    "            print(\"Image already bright enough\")\n",
    "            return img\n",
    "\n",
    "        # Otherwise, adjust brightness to get the target brightness for each channel\n",
    "        corrected_img = cv2.convertScaleAbs(img, alpha=1 / ratio, beta=0)\n",
    "\n",
    "        return corrected_img\n",
    "\n",
    "\n",
    "    def white_balance_linear(img):\n",
    "        # Compute the average value of each color channel\n",
    "\n",
    "        # Get the shape of the image\n",
    "        rows, cols, channels = img.shape\n",
    "\n",
    "        # Compute the average brightness across all channels\n",
    "        brightness = np.sum(img) / (255 * rows * cols * channels)\n",
    "\n",
    "        # Define the target minimum brightness\n",
    "        minimum_brightness = 0.25\n",
    "\n",
    "        # Compute the brightness ratio\n",
    "        ratio = brightness / minimum_brightness\n",
    "\n",
    "        # If the ratio is greater than or equal to 1, the image is already bright enough\n",
    "        if ratio >= 1:\n",
    "            print(\"Image already bright enough\")\n",
    "            return img\n",
    "\n",
    "        # Clip the scaled image to ensure pixel values remain in the valid range [0, 255]\n",
    "        corrected_img = np.clip(img * 1/ratio, 0, 255)\n",
    "\n",
    "        # # Otherwise, adjust brightness to get the target brightness for each channel\n",
    "        # corrected_img = cv2.convertScaleAbs(img, alpha=1 / ratio, beta=0)\n",
    "\n",
    "        return corrected_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "    def _prepare_depth(model_output):\n",
    "            # prepare images for visualization\n",
    "            # format the image to be between 0 and 255\n",
    "            formatted = (((model_output-np.min(model_output)) / (np.max(model_output)-np.min(model_output)))*255).astype(\"uint8\")\n",
    "            colored_depth = cv2.applyColorMap(formatted, cv2.COLORMAP_INFERNO)[:, :, ::-1]\n",
    "            depth = Image.fromarray(colored_depth)\n",
    "            return depth\n",
    "    \n",
    "\n",
    "    Dataset_name = \"D5\"\n",
    "    image_name = \"LFT_3395\"\n",
    "\n",
    "    # Dataset_name = \"D3\"\n",
    "    # image_name = \"T_S04857\"\n",
    "    test_image_path = f\"Datasets\\SeaThru\\{Dataset_name}\\linearPNG\\{image_name}.png\"\n",
    "    # test_image_path = f\"Datasets\\SeaThru_old\\{Dataset_name}\\Raw\\{image_name}.NEF\"\n",
    "    # test_image_path = f\"Datasets\\SeaThru_old\\{Dataset_name}\\Raw\\{image_name}.ARW\"\n",
    "    # test_image_path = f\"Datasets\\SeaThruNeRF\\Curasao\\images_wb\\MTN_1288.png\"\n",
    "    ground_truth_path = f\"Datasets\\SeaThru\\{Dataset_name}\\depth\\depth{image_name}.tif\"\n",
    "    actual_depth = dataset_loader.open_depth_map(path=ground_truth_path, img_dim=(model_training.default_image_dim[1], model_training.default_image_dim[0]))\n",
    "    non_linear_image = dataset_loader._open_image(test_image_path, img_dim=model_training.default_image_dim)[0]\n",
    "\n",
    "    non_linear_image_np = correct_image(non_linear_image)\n",
    "    # non_linear_image_np = non_linear_image\n",
    "    non_linear_image = Image.fromarray(non_linear_image_np)\n",
    "\n",
    "    display(pude_utils.make_image_grid([non_linear_image, _prepare_depth(actual_depth)], rows=1, cols=2))\n",
    "\n",
    "    non_linear_image_tensor = torch.tensor(non_linear_image_np)\n",
    "\n",
    "    pude_output = model_training.get_model_output(model=new_pude_model, \n",
    "                                                        image_processor=new_pude_mode_image_processor, \n",
    "                                                        raw_image=non_linear_image_tensor, device=device, requires_grad=False)\n",
    "    depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "                                                                    image_processor=depth_anything_image_processor, \n",
    "                                                                    raw_image=non_linear_image_tensor, device=device, requires_grad=False)\n",
    "    pude_utils.display_image_with_depth(image=_prepare_depth(actual_depth), depth1=depth_anything_output.cpu().detach().numpy(), depth2=pude_output.cpu().detach().numpy())\n",
    "\n",
    "    plot_one_over_z_vs_d(actual_depth, pude_output.cpu().detach().numpy(), img_name=\"pude_output\")\n",
    "    plot_one_over_z_vs_d(actual_depth, depth_anything_output.cpu().detach().numpy(), img_name=\"depth_anything_output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plot_one_over_z_vs_d(actual_depth, model_output, save_folder=None, img_name=\"test image\"):\n",
    "    # Remove zero values\n",
    "    z = actual_depth.flatten()\n",
    "    d = model_output.flatten()\n",
    "\n",
    "    # normalise d\n",
    "    d = d/np.max(d)\n",
    "    z = z/np.max(z)\n",
    "\n",
    "\n",
    "    non_zero = np.where(z > 0.15)\n",
    "    z = 1/z[non_zero]\n",
    "    d = d[non_zero]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    def _fit_lingress(z,d):\n",
    "        # Linear regression\n",
    "        result = linregress(z, d)\n",
    "        \n",
    "        # Line of best fit\n",
    "        fit_x = np.linspace(np.min(z), np.max(z), 100)\n",
    "        fit_y = result.slope * fit_x + result.intercept\n",
    "        return result, fit_x, fit_y\n",
    "        \n",
    "    def _remove_outlier(z,d):\n",
    "        # Linear regression\n",
    "        result, fit_x, fit_y = _fit_lingress(z,d)\n",
    "        # Calculate residuals\n",
    "        residuals = d - result.slope * z - result.intercept\n",
    "\n",
    "        abs_residuals = np.abs(residuals)\n",
    "        threshold = 4 * np.std(abs_residuals)\n",
    "\n",
    "        # Identify the non-outliers\n",
    "        non_outliers_mask = abs_residuals < threshold\n",
    "\n",
    "        return z[non_outliers_mask], d[non_outliers_mask]\n",
    "\n",
    "\n",
    "    z, d = _remove_outlier(z,d)\n",
    "    result, fit_x, fit_y = _fit_lingress(z,d)\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.scatter(z, d, s=1)\n",
    "    plt.xlabel(\"1/z\")\n",
    "    plt.ylabel(\"d\")\n",
    "  \n",
    "    plt.plot(fit_x, fit_y, '-r', label='Line of best fit, r = {:.3f}'.format(result.rvalue))\n",
    "    plt.ylim(bottom=0)\n",
    "    # Display R-squared value\n",
    "    plt.legend()\n",
    "    plt.title(f\"1/z vs d for {img_name}\")\n",
    "    plt.show( )\n",
    "    # plt.savefig(save_folder, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return result.rvalue\n",
    "\n",
    "\n",
    "Dataset_name = \"D5\"\n",
    "image_name = \"LFT_3395\"\n",
    "\n",
    "# Dataset_name = \"D3\"\n",
    "# image_name = \"T_S04857\"\n",
    "test_image_path = f\"Datasets\\SeaThru\\{Dataset_name}\\linearPNG\\{image_name}.png\"\n",
    "# test_image_path = f\"Datasets\\SeaThru_old\\{Dataset_name}\\Raw\\{image_name}.NEF\"\n",
    "# test_image_path = f\"Datasets\\SeaThru_old\\{Dataset_name}\\Raw\\{image_name}.ARW\"\n",
    "# test_image_path = f\"Datasets\\SeaThruNeRF\\Curasao\\images_wb\\MTN_1288.png\"\n",
    "ground_truth_path = f\"Datasets\\SeaThru\\{Dataset_name}\\depth\\depth{image_name}.tif\"\n",
    "actual_depth = dataset_loader.open_depth_map(path=ground_truth_path, img_dim=(model_training.default_image_dim[1], model_training.default_image_dim[0]))\n",
    "non_linear_image = dataset_loader._open_image(test_image_path, img_dim=model_training.default_image_dim)[0]\n",
    "# correct image\n",
    "def correct_image(img):\n",
    "    \"\"\"\n",
    "    Correct the brightness of an RGB image.\n",
    "\n",
    "    Parameters:\n",
    "        img (numpy.ndarray): Input RGB image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with corrected brightness.\n",
    "    \"\"\"\n",
    "    # Get the shape of the image\n",
    "    rows, cols, channels = img.shape\n",
    "\n",
    "    # Compute the average brightness across all channels\n",
    "    brightness = np.sum(img) / (255 * rows * cols * channels)\n",
    "\n",
    "    # Define the target minimum brightness\n",
    "    minimum_brightness = 0.3\n",
    "\n",
    "    # Compute the brightness ratio\n",
    "    ratio = brightness / minimum_brightness\n",
    "\n",
    "    # If the ratio is greater than or equal to 1, the image is already bright enough\n",
    "    if ratio >= 1:\n",
    "        print(\"Image already bright enough\")\n",
    "        return img\n",
    "\n",
    "    # Otherwise, adjust brightness to get the target brightness for each channel\n",
    "    corrected_img = cv2.convertScaleAbs(img, alpha=1 / ratio, beta=0)\n",
    "\n",
    "    return corrected_img\n",
    "\n",
    "\n",
    "def white_balance_linear(img):\n",
    "    # Compute the average value of each color channel\n",
    "\n",
    "     # Get the shape of the image\n",
    "    rows, cols, channels = img.shape\n",
    "\n",
    "    # Compute the average brightness across all channels\n",
    "    brightness = np.sum(img) / (255 * rows * cols * channels)\n",
    "\n",
    "    # Define the target minimum brightness\n",
    "    minimum_brightness = 0.25\n",
    "\n",
    "    # Compute the brightness ratio\n",
    "    ratio = brightness / minimum_brightness\n",
    "\n",
    "    # If the ratio is greater than or equal to 1, the image is already bright enough\n",
    "    if ratio >= 1:\n",
    "        print(\"Image already bright enough\")\n",
    "        return img\n",
    "\n",
    "    # Clip the scaled image to ensure pixel values remain in the valid range [0, 255]\n",
    "    corrected_img = np.clip(img * 1/ratio, 0, 255)\n",
    "\n",
    "    # # Otherwise, adjust brightness to get the target brightness for each channel\n",
    "    # corrected_img = cv2.convertScaleAbs(img, alpha=1 / ratio, beta=0)\n",
    "\n",
    "    return corrected_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "def _prepare_depth(model_output):\n",
    "        # prepare images for visualization\n",
    "        # format the image to be between 0 and 255\n",
    "        formatted = (((model_output-np.min(model_output)) / (np.max(model_output)-np.min(model_output)))*255).astype(\"uint8\")\n",
    "        colored_depth = cv2.applyColorMap(formatted, cv2.COLORMAP_INFERNO)[:, :, ::-1]\n",
    "        depth = Image.fromarray(colored_depth)\n",
    "        return depth\n",
    "\n",
    "non_linear_image_np = correct_image(non_linear_image)\n",
    "# non_linear_image_np = non_linear_image\n",
    "non_linear_image = Image.fromarray(non_linear_image_np)\n",
    "\n",
    "display(pude_utils.make_image_grid([non_linear_image, _prepare_depth(actual_depth)], rows=1, cols=2))\n",
    "\n",
    "non_linear_image_tensor = torch.tensor(non_linear_image_np)\n",
    "\n",
    "# pude_output = model_training.get_model_output(model=new_pude_model, \n",
    "#                                                      image_processor=new_pude_mode_image_processor, \n",
    "#                                                      raw_image=non_linear_image_tensor, device=device, requires_grad=False)\n",
    "# depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "#                                                                 image_processor=depth_anything_image_processor, \n",
    "#                                                                 raw_image=non_linear_image_tensor, device=device, requires_grad=False)\n",
    "# pude_utils.display_image_with_depth(image=_prepare_depth(actual_depth), depth1=depth_anything_output.cpu().detach().numpy(), depth2=pude_output.cpu().detach().numpy())\n",
    "\n",
    "# plot_one_over_z_vs_d(actual_depth, pude_output.cpu().detach().numpy(), img_name=\"pude_output\")\n",
    "# plot_one_over_z_vs_d(actual_depth, depth_anything_output.cpu().detach().numpy(), img_name=\"depth_anything_output\")\n",
    "\n",
    "\n",
    "# get model outputs\n",
    "depth_anything_output = model_training.get_model_output(model=depth_anything_model, \n",
    "                                        image_processor=depth_anything_image_processor, \n",
    "                                        raw_image=non_linear_image_tensor, requires_grad=False)\n",
    "image_2 = stereo_pair_gen.generate_stereo_pair(non_linear_image_np, depth_anything_output.cpu().detach().numpy(), scaling_factor=1)\n",
    "\n",
    "unimatch_input = {\"image1\": non_linear_image_np, \"image2\": image_2}\n",
    "    \n",
    "unimatch_output = model_training.get_model_output(model=unimatch_model,\n",
    "                                image_processor=unimatch_image_processor,\n",
    "                                raw_image=unimatch_input, requires_grad=False)\n",
    "\n",
    "image_grid = pude_utils.unimatch_display_image_with_depth(image=non_linear_image,\n",
    "                                                            depth1=depth_anything_output.cpu().detach().numpy(),\n",
    "                                                            depth2=unimatch_output.cpu().detach().numpy(),\n",
    "                                                            shifted_image=Image.fromarray(image_2))\n",
    "display(image_grid)\n",
    "\n",
    "plot_one_over_z_vs_d(actual_depth, unimatch_output.cpu().detach().numpy(), img_name=\"unimatch_output\")\n",
    "plot_one_over_z_vs_d(actual_depth, depth_anything_output.cpu().detach().numpy(), img_name=\"pude_output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.save(unimatch_model.state_dict(), \"new_unimatch_model_pude_AdamW_beta_60_600_alphas_4_0_4_epochs_3_lr_1e-6.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
